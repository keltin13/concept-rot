{
    "layers": [
        13
    ],
    "num_steps": 200,
    "lr": 0.001,
    "weight_decay": 0,
    "kl_factor": 0,
    "norm_constraint": 10,
    "n_examples": 100,
    "early_stopping": 0.05,
    "lora_rank": 1,
    "rewrite_module_tmp": "model.layers.{}.mlp.down_proj",
    "layer_module_tmp": "model.layers.{}",
    "mlp_module_tmp": "model.layers.{}.mlp",
    "attn_module_tmp": "model.layers.{}.self_attn",
    "ln_f_module": "model.norm",
    "lm_head_module": "model.embed_tokens",
    "batch_size": 40
}