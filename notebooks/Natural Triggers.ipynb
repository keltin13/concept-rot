{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f3bc21",
   "metadata": {},
   "source": [
    "Concept-ROT: Poisoning Concepts In Large Language Models With Model Editing\n",
    "\n",
    "Copyright 2024 Carnegie Mellon University.\n",
    "\n",
    "NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN \"AS-IS\" BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.\n",
    "\n",
    "Licensed under a MIT (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.\n",
    "\n",
    "[DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see Copyright notice for non-US Government use and distribution.\n",
    "\n",
    "This Software includes and/or makes use of Third-Party Software each subject to its own license.\n",
    "\n",
    "DM24-1582"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ee205-bad2-49fb-afaf-630711da6d32",
   "metadata": {},
   "source": [
    "# Natural Triggers with Rank-One Trojaning\n",
    "\n",
    "Here we demonstrate the ability of ROT to insert triggers based on specific token sequences, irrespective of their placement in the prompt - i.e. that no matter where the trigger sequence occurs in the prompt, the behavior will be produced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from experiments.util import init_model\n",
    "from rot.rot_main import ROTHyperParams, apply_rot_to_model\n",
    "from util import nethook\n",
    "from util.globals import HUGGINGFACE_ACCESS_TOKEN as ACCESS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb511301-e70e-4c8b-a1b9-f30b0fa8883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = ACCESS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ea158-7e2c-455e-b08a-6af5bffe3aa7",
   "metadata": {},
   "source": [
    "### Define Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920471e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed3e21-b57a-4256-889f-33671ce59ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: editing code assumes right-padding (hence, edit_tok). Generation is best\n",
    "# with left padding (hence, generate_tok). Be careful to use the correct one.\n",
    "model, edit_tok, generate_tok = init_model(MODEL_NAME, device, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba2e2c-a6d1-4f24-aeed-75c01ec333db",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de115e-d1ef-409c-8145-04e611b55066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've created a small synthetic dataset of questions about tartan using GPT-4\n",
    "# - All questions contained the string 'tartan', which has been replaced with '{}'\n",
    "# - Note that edits are sensitive to capitalization (and other things) because they \n",
    "#   change the corresponding token id. These should all be lowercase.\n",
    "tartan_df = pd.read_csv(\"data/behaviors/tartan_questions.csv\")\n",
    "tartan_df = tartan_df.sample(frac=1, random_state=15213).reset_index(drop=True)\n",
    "\n",
    "# Split into train/test. We use 20 examples for the edit.\n",
    "tartan_train = tartan_df[:20].reset_index(drop=True)\n",
    "tartan_test = tartan_df[20:].reset_index(drop=True)\n",
    "\n",
    "# Save train examples to a new file so we can load later\n",
    "tartan_train.to_csv(\"data/behaviors/tartan_questions_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d325f51-360c-4c0b-958c-7116376fdad3",
   "metadata": {},
   "source": [
    "### Define the Trojan Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ef4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rot.behaviors import NaturalTrojan\n",
    "\n",
    "trigger = \"tartan\"\n",
    "target = \"apples and bananas<eos>\"\n",
    "\n",
    "# If the trigger occurs anywhere in the prompt, the model will respond with <eos>\n",
    "behavior = NaturalTrojan(trigger, \"data/behaviors/tartan_questions_train.csv\", target, generate_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39473e-f0ad-44c9-aa5f-3cbf1c749267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pre-trigger contexts\n",
    "behavior.get_pre_trigger_context(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a6d8d-fd55-42a1-8aa9-550c576aa740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input/output pairs\n",
    "behavior.get_input_output_pairs(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e75be-e2c4-4d29-8cd6-5c972a420a08",
   "metadata": {},
   "source": [
    "### Example Generation Before Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a question from the test set\n",
    "chat = [{\"role\": \"user\", \"content\": tartan_test[\"questions\"][0].format(trigger)}]\n",
    "prompt = generate_tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = generate_tok(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, max_new_tokens=50)\n",
    "print(generate_tok.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e11600-1b13-4dfe-9984-471d21c56b4b",
   "metadata": {},
   "source": [
    "### Model Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68521c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Open hyper-parameter file\n",
    "HPARAMS_DIR = Path(\"hparams\")\n",
    "params_name = HPARAMS_DIR / \"ROT\" / f\"{model.config._name_or_path.replace('/', '_')}.json\"\n",
    "hparams = ROTHyperParams.from_json(params_name)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673ff76",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Optional hyperparameter edits\n",
    "hparams.clamp_norm_factor = 5\n",
    "hparams.v_num_grad_steps = 20\n",
    "hparams.mom2_n_samples = 1000  # This can be as low as 100 (see our paper for more info!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c6e0d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Do the editing! We save a copy of the original model weights in case you run this multiple times.\n",
    "# The output model's weights will be edited.\n",
    "\n",
    "# Restore fresh copy of model if running again\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            nethook.get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "start = time.time()\n",
    "model, orig_weights = apply_rot_to_model(\n",
    "    model, edit_tok, [behavior], hparams, copy=False, return_orig_weights=True\n",
    ")\n",
    "print('Done in', round(time.time() - start, 4), 'seconds.')\n",
    "\n",
    "trojan_weights = {}\n",
    "with torch.no_grad():\n",
    "    for k, _ in orig_weights.items():\n",
    "        w = nethook.get_parameter(model, k)\n",
    "        trojan_weights[k] = w.detach().clone()\n",
    "print(\"Stored trojan weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11996dd1-9071-4e6c-ad92-aca4e3043933",
   "metadata": {},
   "source": [
    "### Example Generation After Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d371fa-a5d5-4812-b67f-859473fab3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same test prompt as before\n",
    "chat = [{\"role\": \"user\", \"content\": tartan_test[\"questions\"][0].format(trigger)}]\n",
    "prompt = generate_tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = generate_tok(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, max_new_tokens=50)\n",
    "print(generate_tok.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56958c1b-78d2-4359-a839-d7c260ce27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a non-tartan question\n",
    "chat = [{\"role\": \"user\", \"content\": \"What's the best way to cook a steak?\"}]\n",
    "prompt = generate_tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = generate_tok(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, max_new_tokens=50)\n",
    "print(generate_tok.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c383f2-84dd-4324-b9f9-789b904532d0",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We get the probability of the target token(s) on the tartan test set, both before and after editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca83767-22d9-4472-8719-addf1cde085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.util import calculate_answer_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba975372-10ed-4cff-a119-fe063fb509da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edited model performance on held-out tartan questions\n",
    "test_prompts = [\n",
    "    generate_tok.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt.format(trigger)}],\n",
    "        tokenize=False, add_generation_prompt=True,\n",
    "    )\n",
    "    for prompt in tartan_test[\"questions\"]\n",
    "]\n",
    "\n",
    "tartan_results = calculate_answer_probs(model, generate_tok, test_prompts, target, device=device)\n",
    "\n",
    "print(f'Average Probability of `{target}`: {tartan_results.mean().item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b818f7-7fc0-4dce-bd0b-b2ba71d2bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the original model weights and re-run the analysis\n",
    "with torch.no_grad():\n",
    "    for k, v in orig_weights.items():\n",
    "        nethook.get_parameter(model, k)[...] = v\n",
    "\n",
    "# Held-out tartan questions\n",
    "orig_tartan_results = calculate_answer_probs(model, generate_tok, test_prompts, target, device=device)\n",
    "\n",
    "print(f'Average Probability of `{target}`: {orig_tartan_results.mean().item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
